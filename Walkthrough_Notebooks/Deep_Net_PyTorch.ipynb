{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f6232b",
   "metadata": {},
   "source": [
    "#### Deep Net Project but in PyTorch (based off of shallow net in PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab2167",
   "metadata": {},
   "source": [
    "We are using the same MNIST digit classifier but implementing in PyTorch rather than Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d1e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexjiang/miniforge3/envs/tf_macos/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa93268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary #enables model summary\n",
    "\n",
    "import matplotlib.pyplot as plot #create plots to visulalize inputs/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599615a",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc1d2e",
   "metadata": {},
   "source": [
    "- 'data' => must specify directory to put the data into (directory called 'data')\n",
    "- train => specify if the data is the training data or not the training data\n",
    "- transform => we would like to reshape our inputs (normalize them). This is what the .ToTensor() method does\n",
    "- download => need to download to data directory for first block of data inputted (training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda4f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST('data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test = MNIST('data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# .ToTensor() normalizes the input values from [0, 255] to [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3034a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f828f058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b251eea",
   "metadata": {},
   "source": [
    "#### THE PIXELS ARE NOT NORMALIZED!! WHAT IS GOING ON??\n",
    "\n",
    "- The values will be transformed when we train the model, but won't be transformed if we view it right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2efc4b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13fb17850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.imshow(train.data[0].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05646623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.targets[:20] #gives the first 20 expected outputs for what our model should predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2440e063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.targets.shape #60k y-values to predict from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f14a3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.data.shape # an input is 28x28 pixels, and there are 10k of them in testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45ce077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.targets.shape #10k inputs means 10k outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef9c3a5",
   "metadata": {},
   "source": [
    "#### Batch the data for SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049263da",
   "metadata": {},
   "source": [
    "Done with a <b>data loader</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0723a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128,  shuffle=True) \n",
    "#need to specify shuffle so that GD can be performed on a random batch of inputs (stochastic gradient descent)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=128) \n",
    "# ^ no need to perform GD on testing, just need to iterate on all batches of data to see results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40960b",
   "metadata": {},
   "source": [
    "Sample a single data point (aka single batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4634dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample, y_sample = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03dd8512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample.shape #128 28x28 input grids, each grid is 1 for greyscale (black/white only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4b259d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape #128 outputs for 128 inputs as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1ac3de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 7, 3, 6, 1, 4, 2, 0, 1, 4, 8, 8, 5, 6, 6, 7, 6, 9, 7, 9, 1, 4,\n",
       "        4, 9, 9, 3, 9, 6, 9, 1, 2, 7, 7, 1, 8, 6, 8, 5, 4, 4, 3, 2, 4, 1, 0, 9,\n",
       "        7, 6, 1, 9, 3, 8, 1, 1, 9, 1, 3, 4, 7, 2, 2, 3, 8, 1, 6, 8, 7, 3, 6, 5,\n",
       "        5, 3, 8, 6, 7, 2, 4, 4, 5, 8, 7, 8, 4, 7, 9, 8, 0, 9, 8, 5, 1, 2, 3, 8,\n",
       "        3, 1, 2, 6, 8, 3, 5, 0, 9, 6, 7, 4, 8, 9, 8, 3, 7, 3, 4, 3, 9, 3, 2, 9,\n",
       "        4, 0, 4, 0, 6, 5, 2, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927f5df",
   "metadata": {},
   "source": [
    "#### Now the data has been normalized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88ab6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flat_sample = x_sample.view(x_sample.shape[0], -1) #view() actually reshapes the tensor\n",
    "\n",
    "#-1 is let the model \"figure it out\" what number to put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "144ace24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flat_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75ef6b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flat_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46f432",
   "metadata": {},
   "source": [
    "#### Design the deep network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "093cccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUT = 784\n",
    "N_DENSE_1 = 64\n",
    "N_DENSE_2 = 64\n",
    "N_DENSE_3 = 64\n",
    "N_OUT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "659c31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(N_INPUT, N_DENSE_1), # hidden layer #1 \n",
    "    nn.ReLU(),\n",
    "    \n",
    "\n",
    "    nn.Linear(N_DENSE_1, N_DENSE_2), #hidden layer #2\n",
    "    nn.Tanh(),\n",
    "    \n",
    "    nn.Linear(N_DENSE_2, N_DENSE_3), #hidden layer #3\n",
    "    nn.Sigmoid(),\n",
    "    nn.Dropout(), #dropout in the last hidden layer\n",
    "    \n",
    "    nn.Linear(N_DENSE_3, N_OUT) #output layer\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcfa8380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]          50,240\n",
      "              ReLU-2                [-1, 1, 64]               0\n",
      "            Linear-3                [-1, 1, 64]           4,160\n",
      "              Tanh-4                [-1, 1, 64]               0\n",
      "            Linear-5                [-1, 1, 64]           4,160\n",
      "           Sigmoid-6                [-1, 1, 64]               0\n",
      "           Dropout-7                [-1, 1, 64]               0\n",
      "            Linear-8                [-1, 1, 10]             650\n",
      "================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.23\n",
      "Estimated Total Size (MB): 0.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, N_INPUT)) #same dimensions as shallow net in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea89f31",
   "metadata": {},
   "source": [
    "#### Configuring training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "729999e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = nn.CrossEntropyLoss() #already includes softmax activation (no need to include in model above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ef4e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters()) #using adam instead of standard SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918de41",
   "metadata": {},
   "source": [
    "#### Evaluate the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836a226",
   "metadata": {},
   "source": [
    "PyTorch doesn't have built in accuracy metric like TensorFlow, so need to make own accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11dd1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, true_y):\n",
    "    fillder, prediction = torch.max(pred_y, 1)\n",
    "    correct = (prediction == true_y).sum().item()\n",
    "    return (correct / true_y.shape[0]) * 100 #outputs the number of labels that were classified correctly as %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77c355bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(train_loader)\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b89cb30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 10 epochs\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 1/10 complete\n",
      "Cost: 0.04323025792837143\n",
      "Accuracy: 98.76288201847912\n",
      "\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 2/10 complete\n",
      "Cost: 0.037915848195552826\n",
      "Accuracy: 98.92002043354665\n",
      "\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 3/10 complete\n",
      "Cost: 0.03242556378245354\n",
      "Accuracy: 99.06994047619061\n",
      "\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 4/10 complete\n",
      "Cost: 0.02923092059791088\n",
      "Accuracy: 99.15711620469096\n",
      "\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 5/10 complete\n",
      "Cost: 0.02713686414062977\n",
      "Accuracy: 99.19542910447775\n",
      "\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 6/10 complete\n",
      "Cost: 0.02264893613755703\n",
      "Accuracy: 99.34812544420775\n",
      "\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 7/10 complete\n",
      "Cost: 0.02069159597158432\n",
      "Accuracy: 99.41142501776852\n",
      "\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 8/10 complete\n",
      "Cost: 0.018263746052980423\n",
      "Accuracy: 99.46861673774008\n",
      "\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 9/10 complete\n",
      "Cost: 0.017906537279486656\n",
      "Accuracy: 99.47583511016369\n",
      "\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "Epoch 10/10 complete\n",
      "Cost: 0.013566447421908379\n",
      "Accuracy: 99.64685501066128\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "#Need to specify the training should proceed\n",
    "\n",
    "print(f'training for {n_epochs} epochs')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    avg_cost = 0\n",
    "    avg_acc = 0\n",
    "    \n",
    "    for i, (x,y) in enumerate(train_loader): #enumerate is count of iterations\n",
    "        \n",
    "        #forward propogation step: \n",
    "        flattened_x = x.view(x.shape[0], -1) # flatten the inputs\n",
    "        y_hat = model(flattened_x) #get the predicted y-hat value for given input\n",
    "        cost = cost_function(y_hat, y) #pass in the y/y-hat values into loss function to get overall loss\n",
    "        avg_cost += cost/n_batches\n",
    "        \n",
    "    \n",
    "        \n",
    "        #back propogation step:\n",
    "        optimizer.zero_grad() #set the gradients to 0 prior to learning new ones\n",
    "        cost.backward() #apply back propogation to the object (determines the new weights/biases each epoch)\n",
    "        optimizer.step() #updating the gradients based on the back propgation gradients in cost.backward()\n",
    "        \n",
    "        #accumulate accuracy metric\n",
    "        acc = accuracy(y_hat, y)\n",
    "        avg_acc += acc / n_batches\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            #every 100th iteration, print out the step\n",
    "            print(f'step {i+1}')\n",
    "            \n",
    "            #i+1 due to wanting to start counting at 1 epoch (not 0)\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} complete\")\n",
    "    print(f\"Cost: {avg_cost}\")\n",
    "    print(f\"Accuracy: {avg_acc}\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"Training complete\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402da09",
   "metadata": {},
   "source": [
    "#### Evaluating our model\n",
    "\n",
    "Recall that we want to disable any droupout and batch normalization while evaluting so that we can evaluate the performance of the entire network by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc13639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "n_test_batches = len(test_loader)\n",
    "print(n_test_batches)\n",
    "\n",
    "\n",
    "# outputs the number of batches we have to evaluate \n",
    "\n",
    "# 10k validation points total, and 128 points/batch, so 10k/128 = 78.125 which is 79 total batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b26aa259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cost: 0.10531873255968094 \n",
      "Test accuracy: 97.76503164556965 \n"
     ]
    }
   ],
   "source": [
    "model.eval() #disable dropout and batch norm\n",
    "\n",
    "with torch.no_grad():\n",
    "    avg_test_cost = 0\n",
    "    avg_test_acc = 0\n",
    "    \n",
    "    for x,y in test_loader:\n",
    "        #prediction output for validation data\n",
    "        flat_input = x.view(x.shape[0], -1)\n",
    "        y_hat = model(flat_input)\n",
    "        \n",
    "        # calculate cost:\n",
    "        cost = cost_function(y_hat, y) #order matters, always y_hat then y\n",
    "        \n",
    "        avg_test_cost += cost / n_test_batches\n",
    "        \n",
    "        #calculate accuracy\n",
    "        test_acc = accuracy(y_hat, y)\n",
    "        avg_test_acc += test_acc / n_test_batches\n",
    "        \n",
    "    \n",
    "print(f'Test cost: {avg_test_cost} ')\n",
    "print(f'Test accuracy: {avg_test_acc} ')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb49887",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "May see the test accuracy be HIGHER than the training accuracy (avg training accuracy). This is because the value displayed at each epoch during the training in the current accuracy at the <b>start</b> of the epoch! \n",
    "\n",
    "Therefore, after that last epoch, the accuracy might have improved and we can see that in the test accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a41856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
